"""
å®æ—¶é¢„æµ‹å¼•æ“é¡µé¢ - Phase 5e
æ”¯æŒæµå¼é¢„æµ‹ã€å®æ—¶ç²¾åº¦è¯„ä¼°ã€æ‰¹é‡æ¨ç†
"""

import streamlit as st
import requests
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from datetime import datetime, timedelta

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å®æ—¶é¢„æµ‹ | GeologAI",
    page_icon="âš¡",
    layout="wide"
)

# API é…ç½®
API_BASE_URL = "http://127.0.0.1:8000"
PREDICT_ENDPOINT = f"{API_BASE_URL}/api/predictions"

# éªŒè¯è®¤è¯
if not st.session_state.get("auth_token"):
    st.error("âŒ è¯·å…ˆç™»å½•")
    st.stop()

st.title("âš¡ å®æ—¶é¢„æµ‹å¼•æ“")
st.markdown("---")

headers = {
    "Authorization": f"Bearer {st.session_state.auth_token}",
    "Content-Type": "application/json"
}

# ======================== ä¾§è¾¹æ é…ç½® ========================
with st.sidebar:
    st.subheader("âš™ï¸ é¢„æµ‹é…ç½®")
    
    # é¢„æµ‹æ¨¡å¼
    prediction_mode = st.radio(
        "é¢„æµ‹æ¨¡å¼",
        ["å®æ—¶æµå¼", "æ‰¹é‡æ¨ç†", "å•æ ·æœ¬é¢„æµ‹"]
    )
    
    st.markdown("---")
    
    # æ¨¡å‹é€‰æ‹©
    model_id = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["model_dl_v1", "model_lstm_v2", "model_ensemble_v3"],
        format_func=lambda x: {"model_dl_v1": "æ·±åº¦å­¦ä¹  v1", 
                               "model_lstm_v2": "LSTM v2", 
                               "model_ensemble_v3": "é›†æˆæ¨¡å‹ v3"}.get(x)
    )
    
    st.markdown("---")
    
    # æ€§èƒ½ç›‘æ§
    st.subheader("ğŸ“Š æ€§èƒ½ç›‘æ§")
    
    show_latency = st.checkbox("æ˜¾ç¤ºå»¶è¿Ÿ", value=True)
    show_confidence = st.checkbox("æ˜¾ç¤ºç½®ä¿¡åº¦", value=True)
    show_throughput = st.checkbox("æ˜¾ç¤ºååé‡", value=True)

# ======================== å®æ—¶æµå¼é¢„æµ‹ ========================
if prediction_mode == "å®æ—¶æµå¼":
    st.subheader("ğŸ“¡ å®æ—¶æµå¼é¢„æµ‹")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if st.button("â–¶ï¸ å¯åŠ¨é¢„æµ‹æµ", use_container_width=True):
            st.info("å®æ—¶é¢„æµ‹æµå·²å¯åŠ¨ï¼Œç­‰å¾…æ•°æ®...")
            
            # åˆå§‹åŒ–æŒ‡æ ‡å®¹å™¨
            metrics_placeholder = st.empty()
            chart_placeholder = st.empty()
            table_placeholder = st.empty()
            
            # æ¨¡æ‹Ÿæµæ•°æ®
            predictions_buffer = []
            latencies = []
            
            for i in range(20):  # æ¨¡æ‹Ÿ 20 ä¸ªé¢„æµ‹
                try:
                    # ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹
                    pred = {
                        "timestamp": datetime.now().isoformat(),
                        "input": np.random.randn(5).tolist(),
                        "prediction": np.random.rand(),
                        "confidence": np.random.uniform(0.7, 1.0),
                        "latency_ms": np.random.uniform(10, 100)
                    }
                    
                    predictions_buffer.append(pred)
                    latencies.append(pred["latency_ms"])
                    
                    # æ›´æ–°æŒ‡æ ‡
                    with metrics_placeholder.container():
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("æ€»é¢„æµ‹æ•°", len(predictions_buffer))
                        with col2:
                            st.metric("å¹³å‡å»¶è¿Ÿ", f"{np.mean(latencies):.1f}ms")
                        with col3:
                            st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{np.mean([p['confidence'] for p in predictions_buffer]):.3f}")
                        with col4:
                            st.metric("ååé‡", f"{len(predictions_buffer) / ((i+1) * 0.1):.1f} é¢„æµ‹/ç§’")
                    
                    # æ›´æ–°å®æ—¶å›¾è¡¨
                    if len(predictions_buffer) > 1:
                        fig = go.Figure()
                        
                        fig.add_trace(go.Scatter(
                            y=[p["prediction"] for p in predictions_buffer],
                            mode='lines+markers',
                            name='é¢„æµ‹å€¼',
                            line=dict(color='blue')
                        ))
                        
                        fig.update_layout(
                            title="å®æ—¶é¢„æµ‹æµ",
                            xaxis_title="é¢„æµ‹åºå·",
                            yaxis_title="é¢„æµ‹å€¼",
                            height=400
                        )
                        
                        with chart_placeholder:
                            st.plotly_chart(fig, use_container_width=True)
                    
                    # æ›´æ–°é¢„æµ‹è¡¨æ ¼
                    if len(predictions_buffer) > 0:
                        display_df = pd.DataFrame(predictions_buffer[-10:])
                        with table_placeholder:
                            st.dataframe(display_df, use_container_width=True)
                    
                    import time
                    time.sleep(0.1)
                
                except Exception as e:
                    st.error(f"é¢„æµ‹æµé”™è¯¯: {str(e)}")
                    break

# ======================== æ‰¹é‡æ¨ç† ========================
elif prediction_mode == "æ‰¹é‡æ¨ç†":
    st.subheader("ğŸ“¦ æ‰¹é‡æ¨ç†")
    
    col1, col2 = st.columns(2)
    
    with col1:
        batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 10, 10000, 100, 10)
    
    with col2:
        num_batches = st.number_input("æ‰¹æ¬¡æ•°é‡", 1, 100, 5)
    
    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†", use_container_width=True):
        with st.spinner("æ­£åœ¨æ‰§è¡Œæ‰¹é‡æ¨ç†..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            all_results = []
            total_samples = batch_size * num_batches
            
            for batch_idx in range(num_batches):
                try:
                    # æ¨¡æ‹Ÿæ¨ç†è¯·æ±‚
                    batch_data = {
                        "model_id": model_id,
                        "batch_size": batch_size,
                        "num_samples": total_samples
                    }
                    
                    # æ¨¡æ‹Ÿç»“æœ
                    batch_results = {
                        "batch_idx": batch_idx,
                        "predictions": np.random.rand(batch_size).tolist(),
                        "latency_ms": np.random.uniform(50, 200)
                    }
                    
                    all_results.extend(batch_results["predictions"])
                    
                    progress = (batch_idx + 1) / num_batches
                    progress_bar.progress(progress)
                    status_text.text(f"å·²å®Œæˆ: {batch_idx + 1}/{num_batches} æ‰¹æ¬¡ï¼Œæ€»æ ·æœ¬: {(batch_idx + 1) * batch_size}/{total_samples}")
                    
                except Exception as e:
                    st.error(f"æ‰¹é‡æ¨ç†é”™è¯¯: {str(e)}")
                    break
            
            # æ˜¾ç¤ºç»“æœ
            st.success("âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»æ ·æœ¬æ•°", total_samples)
            with col2:
                st.metric("å¹³å‡é¢„æµ‹å€¼", f"{np.mean(all_results):.3f}")
            with col3:
                st.metric("æ ‡å‡†å·®", f"{np.std(all_results):.3f}")
            with col4:
                st.metric("å¤„ç†æ—¶é—´", f"{num_batches * 100:.0f}ms")
            
            # é¢„æµ‹åˆ†å¸ƒ
            fig = go.Figure(data=[
                go.Histogram(x=all_results, nbinsx=50)
            ])
            fig.update_layout(
                title="é¢„æµ‹ç»“æœåˆ†å¸ƒ",
                xaxis_title="é¢„æµ‹å€¼",
                yaxis_title="é¢‘æ•°",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)

# ======================== å•æ ·æœ¬é¢„æµ‹ ========================
elif prediction_mode == "å•æ ·æœ¬é¢„æµ‹":
    st.subheader("ğŸ¯ å•æ ·æœ¬é¢„æµ‹")
    
    col1, col2 = st.columns(2)
    
    with col1:
        num_features = st.number_input("ç‰¹å¾æ•°", 1, 100, 10)
    
    with col2:
        st.markdown("**è¾“å…¥ç‰¹å¾**")
    
    # è¾“å…¥ç‰¹å¾
    features = []
    cols = st.columns(5)
    for i in range(num_features):
        with cols[i % 5]:
            feature_value = st.number_input(
                f"ç‰¹å¾ {i+1}",
                value=0.0,
                format="%.2f"
            )
            features.append(feature_value)
    
    if st.button("ğŸ”® æ‰§è¡Œé¢„æµ‹", use_container_width=True):
        with st.spinner("æ­£åœ¨ç”Ÿæˆé¢„æµ‹..."):
            try:
                # æ¨¡æ‹Ÿé¢„æµ‹è¯·æ±‚
                prediction_request = {
                    "model_id": model_id,
                    "features": features
                }
                
                # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
                prediction_value = np.random.rand()
                confidence = np.random.uniform(0.7, 1.0)
                latency = np.random.uniform(10, 50)
                
                st.success("âœ… é¢„æµ‹å®Œæˆï¼")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("é¢„æµ‹å€¼", f"{prediction_value:.4f}")
                
                with col2:
                    st.metric("ç½®ä¿¡åº¦", f"{confidence:.3f}")
                
                with col3:
                    st.metric("å»¶è¿Ÿ", f"{latency:.1f}ms")
                
                # ä¸ç¡®å®šæ€§ä¼°è®¡
                st.markdown("---")
                st.subheader("ğŸ“Š ä¸ç¡®å®šæ€§ä¼°è®¡")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("ç‚¹ä¼°è®¡å€¼", f"{prediction_value:.4f}")
                
                with col2:
                    # ç½®ä¿¡åŒºé—´
                    lower = prediction_value - 0.1
                    upper = prediction_value + 0.1
                    st.metric("95% ç½®ä¿¡åŒºé—´", f"[{lower:.4f}, {upper:.4f}]")
                
                # å†³ç­–æ”¯æŒ
                st.markdown("---")
                st.subheader("ğŸ¯ å†³ç­–æ”¯æŒ")
                
                if confidence > 0.9:
                    st.success("âœ… é«˜ç½®ä¿¡åº¦é¢„æµ‹ï¼Œå¯ç›´æ¥ä½¿ç”¨")
                elif confidence > 0.7:
                    st.warning("âš ï¸ ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œå»ºè®®å®¡æ ¸åä½¿ç”¨")
                else:
                    st.error("âŒ ä½ç½®ä¿¡åº¦ï¼Œä¸å»ºè®®ä½¿ç”¨")
                
            except Exception as e:
                st.error(f"é¢„æµ‹é”™è¯¯: {str(e)}")

# ======================== é¡µè„š ========================
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    if st.button("ğŸ§  æ·±åº¦å­¦ä¹ ", use_container_width=True):
        st.switch_page("pages/10_deep_learning.py")
with col2:
    if st.button("ğŸ¯ æ¨¡å‹è§£é‡Š", use_container_width=True):
        st.switch_page("pages/12_model_interpretability.py")
with col3:
    if st.button("ğŸ”„ åˆ·æ–°", use_container_width=True):
        st.cache_data.clear()
        st.rerun()
